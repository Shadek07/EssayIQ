{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This cell is just a comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import collections\n",
    "#import constants\n",
    "#import lcs\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "#import bokeh_utils\n",
    "from itertools import compress, product\n",
    "import ast\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "\n",
    "#all different combinations of a list\n",
    "def get_combinations(items):\n",
    "    return ( set(compress(items,mask)) for mask in product(*[[0,1]]*len(items)) )\n",
    "#import mpld3\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (40,40)\n",
    "#plt.locator_params(axis='y', nbins=3)\n",
    "\n",
    "#print(mp.keys())\n",
    "#print(mp.values())\n",
    "\n",
    "essayiq = []\n",
    "cnt = 0\n",
    "\n",
    "essayiqarray = []\n",
    "\n",
    "\n",
    "#filestr = '_1_coach_7sentsize_10dist_0.6run_2'\n",
    "#filestr = '_2_coach_7sentsize_15dist_0.7run_2'\n",
    "filestr = '_3_coach_7sentsize_10dist_0.7run_2'\n",
    "\n",
    "with open('essayiq' + filestr + '.txt') as fp:\n",
    "      cnt = 0\n",
    "      prev = \"\"\n",
    "      for line in fp:\n",
    "        if len(line) < 10:\n",
    "          continue\n",
    "        data = ast.literal_eval(line)\n",
    "        themeid = data['themeid']\n",
    "        essayiqarray.append([cnt, data['sentenceindex'], data['submissionname'], data['themeMarker'],data['sentence'],data['color']])\n",
    "        #essayiq.append([cnt, data['sentenceindex'], data['submissionname'], data['themeMarker'],data['sentence'],data['color']])\n",
    "        if prev != data['submissionname']:\n",
    "            cnt += 8\n",
    "            prev = data['submissionname']\n",
    "#dfessayiq = pd.DataFrame(essayiq,columns=['submission','sentenceindex','submissionname','themeMarker','sentence','color'])         \n",
    "\n",
    "\n",
    "phrase2vecarray = []\n",
    "with open('phrase2vec' + filestr + '.txt') as fp:\n",
    "      cnt = 0\n",
    "      prev = \"\"\n",
    "      for line in fp:\n",
    "        if len(line) < 10:\n",
    "          continue\n",
    "        data = ast.literal_eval(line)\n",
    "        themeid = data['themeid']\n",
    "        phrase2vecarray.append([cnt, data['sentenceindex'], data['submissionname'], data['themeMarker'],data['sentence'],data['color']])\n",
    "        #essayiq.append([cnt, data['sentenceindex'], data['submissionname'], data['themeMarker'],data['sentence'],data['color']])\n",
    "        if prev != data['submissionname']:\n",
    "            cnt += 8\n",
    "            prev = data['submissionname']\n",
    "print(len(phrase2vecarray))\n",
    "goldarray = []\n",
    "with open('essayiqgold' + filestr + '.txt') as fp:\n",
    "      cnt = 2\n",
    "      prev = \"\"\n",
    "      for line in fp:\n",
    "        if len(line) < 10:\n",
    "          continue\n",
    "        data = ast.literal_eval(line)\n",
    "        themeid = data['themeid']\n",
    "        goldarray.append([cnt, data['sentenceindex'], data['submissionname'], data['themeMarker'], data['sentence'],data['color']])\n",
    "        if prev != data['submissionname']:\n",
    "            cnt += 8\n",
    "            prev = data['submissionname']\n",
    "\n",
    "\n",
    "#combined\n",
    "#df = pd.DataFrame(essayiq,columns=['submission','sentenceindex','submissionname','themeMarker','sentence','color']) \n",
    "#only gold standard\n",
    "#dfgold = pd.DataFrame(goldarray,columns=['submission','sentenceindex','submissionname','themeMarker','sentence','color'])\n",
    "\n",
    "#dfessayiq['match'] = np.where(dfessayiq['submissionname'] == dfgold['submissionname'] & dfessayiq['sentenceindex'] == dfgold['sentenceindex'], 'True', 'False')\n",
    "\n",
    "#plot1 = bokeh_utils.scatter_with_hover(df, 'submission', 'sentenceindex',fig_width=1000, fig_height=600, cols=['submission','sentenceindex','submissionname','themeMarker','sentence','color'])\n",
    "#bokeh_utils.draw_multiple_plot(plot1)\n",
    "#print(confusion_matrix(y_true, y_pred, labels=[\"ant\", \"cat\", \"bird\"]))\n",
    "candidate_sentences = dict()\n",
    "with open('candidate_sentences' + filestr + '.txt') as fp:\n",
    "    for line in fp:\n",
    "        if len(line) < 10:\n",
    "          continue\n",
    "        data = ast.literal_eval(line)\n",
    "        candidate_sentences[(data['sentenceindex'], data['submissionname'])] = 1\n",
    "#themelabels = ['Culture in a working environment','Learning','Non-blaming culture','Administrative leadership','Humility']\n",
    "#themelabels = ['Fast Paced environment','Short Staffing','Home life responsibilities','Negative Impact on Patient Safety','Utilization of IAMSAFE Checklist']\n",
    "themelabels = ['Examples of bad designs similar to the \"tricky doors\"','Impacts from bad designs','Barriers to improvement to non-user-friendly design','Mis-attribution of user errors caused by bad designs', 'Workplace examples of bad and good design']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between EssayIQ and Gold Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment settings for USE\n",
      "both hit: 109\n",
      "mismatch when marked by both essayiq and annotator 196\n",
      "both miss: 53\n",
      "marked by only essayiq: 140\n",
      "marked by coach only: 77\n",
      "candidate sentences:  50\n",
      "kappa score:  0.20010704489195164\n",
      "250 250\n",
      "essay level kappa score:  0.3023666548922642\n",
      "essay level kappa score (with weights):  0.30750862297691695\n"
     ]
    }
   ],
   "source": [
    "both_match_with_theme = 0\n",
    "both_match_with_none = 0\n",
    "mismatch = 0\n",
    "marked_by_essayiq_only = 0\n",
    "markedbycoach_only = 0\n",
    "\n",
    "predicted_themes_by_essayiq = []\n",
    "annotator_themes = []\n",
    "\n",
    "candidate_sentence_cnt = 0\n",
    "essay_level_themes_essayiq = dict() #\n",
    "essay_level_themes_annotation = dict()\n",
    "candidate_lines = set() #store line numbers from goldarray\n",
    "for i, row1 in enumerate(essayiqarray):\n",
    "    not_in_golddata = True #check if any sentence was not annotated by coach, but essayiq marked it\n",
    "    for j, row2 in enumerate(goldarray):\n",
    "        if row1[1] == row2[1] and row1[2] == row2[2]: #same sentence from same submission, row1 is essayiq prediction, row2 is annotation labelling \n",
    "            not_in_golddata = False\n",
    "            if (row1[1], row1[2]) in candidate_sentences:\n",
    "                candidate_sentence_cnt += 1\n",
    "                candidate_lines.add(j)\n",
    "                continue\n",
    "            if row1[5] == row2[5]:\n",
    "                both_match_with_theme += 1\n",
    "                predicted_themes_by_essayiq.append(row1[3])\n",
    "                annotator_themes.append(row2[3])\n",
    "                \n",
    "                #essaylevel theme existence\n",
    "                if row1[2] in essay_level_themes_essayiq:\n",
    "                    essay_level_themes_essayiq[row1[2]].append(row1[3])\n",
    "                else:\n",
    "                    essay_level_themes_essayiq[row1[2]] =  [row1[3]] #set([row1[3]])\n",
    "                if row2[2] in essay_level_themes_annotation:\n",
    "                    essay_level_themes_annotation[row2[2]].append(row2[3])\n",
    "                else:\n",
    "                    essay_level_themes_annotation[row2[2]] = [row2[3]] #set([row2[3]])                    \n",
    "            else:\n",
    "                if row1[5] != 'None':\n",
    "                    mismatch += 1\n",
    "                    predicted_themes_by_essayiq.append(row1[3])\n",
    "                    annotator_themes.append(row2[3])                   \n",
    "                    if row1[2] in essay_level_themes_essayiq:\n",
    "                        essay_level_themes_essayiq[row1[2]].append(row1[3])\n",
    "                    else:\n",
    "                        essay_level_themes_essayiq[row1[2]] = [row1[3]] #set([row1[3]])\n",
    "                    if row2[2] in essay_level_themes_annotation:\n",
    "                        essay_level_themes_annotation[row2[2]].append(row2[3])\n",
    "                    else:\n",
    "                        essay_level_themes_annotation[row2[2]] = [row2[3]] #set([row2[3]])\n",
    "                        \n",
    "    if not_in_golddata:\n",
    "        if row1[5] == 'None':\n",
    "            both_match_with_none += 1\n",
    "        else:\n",
    "            marked_by_essayiq_only += 1          \n",
    "for j, row2 in enumerate(goldarray):\n",
    "    if j in candidate_lines:\n",
    "        continue\n",
    "    in_essayiq = False\n",
    "    for i, row1 in enumerate(essayiqarray):\n",
    "        if row1[1] == row2[1] and row1[2] == row2[2]:\n",
    "            in_essayiq = True if row1[5] != 'None' else False\n",
    "            break\n",
    "    if in_essayiq == False:\n",
    "        markedbycoach_only += 1\n",
    "\n",
    "print ('experiment settings for USE')\n",
    "print ('both hit:', both_match_with_theme)\n",
    "print('mismatch when marked by both essayiq and annotator', mismatch) \n",
    "print('both miss:',both_match_with_none)\n",
    "print('marked by only essayiq:', marked_by_essayiq_only)\n",
    "print('marked by coach only:', markedbycoach_only)      \n",
    "print ('candidate sentences: ', candidate_sentence_cnt)\n",
    "\n",
    "\n",
    "mat = confusion_matrix(annotator_themes, predicted_themes_by_essayiq,labels=themelabels)\n",
    "#print(themelabels)\n",
    "#print (mat)\n",
    "'''df_cm = pd.DataFrame(mat, index=themelabels, columns=themelabels)\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}) # font size\n",
    "#plt.show()'''\n",
    "\n",
    "y_true = pd.Series(annotator_themes)\n",
    "y_pred = pd.Series(predicted_themes_by_essayiq)\n",
    "\n",
    "pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "#pd.crosstab(y_true, y_pred, rownames=['True'], colnames=['Predicted']).apply(lambda r: 100.0 * r/r.sum() )\n",
    "cohen_score = cohen_kappa_score(annotator_themes, predicted_themes_by_essayiq)\n",
    "print ('kappa score: ', cohen_score)\n",
    "\n",
    "#essaylevel theme existence experiment\n",
    "thememap = dict()\n",
    "for i,label in enumerate(themelabels):\n",
    "    thememap[label] = i\n",
    "\n",
    "essayiqvalues = []\n",
    "annotatorvalues = []\n",
    "essayiqvalues2 = [] #this array takes care of how many sentences contain a certain theme\n",
    "annotatorvalues2 = [] #this array takes care of how many sentences contain a certain theme\n",
    "for submission in essay_level_themes_essayiq:\n",
    "    arr = [0]*len(themelabels)\n",
    "    arr2 = [0]*len(themelabels)\n",
    "    for theme in essay_level_themes_essayiq[submission]:\n",
    "        arr[thememap[theme]] = 1\n",
    "        arr2[thememap[theme]] += 1\n",
    "    essayiqvalues.extend(arr)\n",
    "    essayiqvalues2.extend(arr2)\n",
    "\n",
    "for submission in essay_level_themes_annotation:\n",
    "    arr = [0]*len(themelabels)\n",
    "    arr2 = [0]*len(themelabels)\n",
    "    for theme in essay_level_themes_annotation[submission]:\n",
    "        arr[thememap[theme]] = 1\n",
    "        arr2[thememap[theme]] += 1\n",
    "    annotatorvalues.extend(arr)\n",
    "    annotatorvalues2.extend(arr2)\n",
    "print (len(essayiqvalues), len(annotatorvalues))\n",
    "cohen_score = cohen_kappa_score(annotatorvalues, essayiqvalues)\n",
    "print ('essay level kappa score: ', cohen_score)\n",
    "\n",
    "cohen_score = cohen_kappa_score(annotatorvalues2, essayiqvalues2, weights='quadratic')\n",
    "print ('essay level kappa score (with weights): ', cohen_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between Phrase2vec and Gold Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experment settings for phrase2vec:\n",
      "both hit: 92\n",
      "mismatch when marked by both phrase2vec and annotator 287\n",
      "both miss: 4\n",
      "marked by only phrase2vec: 189\n",
      "marked by coach only: 3\n",
      "candidate sentences:  50\n",
      "kappa score:  0.0744850119546997\n",
      "250 250\n",
      "essay level kappa score:  0.02137320911297269\n",
      "essay level kappa score (with weights):  0.11720450114470171\n"
     ]
    }
   ],
   "source": [
    "both_match_with_theme = 0\n",
    "both_match_with_none = 0\n",
    "mismatch = 0\n",
    "marked_by_phrase2vec_only = 0\n",
    "markedbycoach_only = 0\n",
    "\n",
    "predicted_themes_by_phrase2vec = []\n",
    "annotator_themes = []\n",
    "\n",
    "essay_level_themes_phrase = dict() #\n",
    "essay_level_themes_annotation = dict()\n",
    "\n",
    "candidate_sentence_cnt = 0\n",
    "candidate_lines = set()\n",
    "for i, row1 in enumerate(phrase2vecarray):\n",
    "    not_in_golddata = True #check if any sentence was not annotated by coach, but phrase2vec marked it\n",
    "    for j, row2 in enumerate(goldarray):\n",
    "        if row1[1] == row2[1] and row1[2] == row2[2]: #same sentence from same submission, row1 is phrase2vec prediction, row2 is annotation labelling \n",
    "            not_in_golddata = False\n",
    "            if (row1[1], row1[2]) in candidate_sentences:\n",
    "                candidate_lines.add(j)\n",
    "                candidate_sentence_cnt += 1\n",
    "                continue\n",
    "            if row1[5] == row2[5]:\n",
    "                both_match_with_theme += 1\n",
    "                predicted_themes_by_phrase2vec.append(row1[3])\n",
    "                annotator_themes.append(row2[3])\n",
    "                \n",
    "                #essaylevel theme existence\n",
    "                if row1[2] in essay_level_themes_phrase:\n",
    "                    essay_level_themes_phrase[row1[2]].append(row1[3])\n",
    "                else:\n",
    "                    essay_level_themes_phrase[row1[2]] = [row1[3]] #set([row1[3]])\n",
    "                if row2[2] in essay_level_themes_annotation:\n",
    "                    essay_level_themes_annotation[row2[2]].append(row2[3])\n",
    "                else:\n",
    "                    essay_level_themes_annotation[row2[2]] = [row2[3]] #set([row2[3]]) \n",
    "            \n",
    "            else:\n",
    "                if row1[5] != 'None':\n",
    "                    mismatch += 1\n",
    "                    predicted_themes_by_phrase2vec.append(row1[3])\n",
    "                    annotator_themes.append(row2[3])\n",
    "                    \n",
    "                    if row1[2] in essay_level_themes_phrase:\n",
    "                        essay_level_themes_phrase[row1[2]].append(row1[3])\n",
    "                    else:\n",
    "                        essay_level_themes_phrase[row1[2]] = [row1[3]] #set([row1[3]])\n",
    "                    if row2[2] in essay_level_themes_annotation:\n",
    "                        essay_level_themes_annotation[row2[2]].append(row2[3])\n",
    "                    else:\n",
    "                        essay_level_themes_annotation[row2[2]] = [row2[3]] #set([row2[3]])\n",
    "            \n",
    "    if not_in_golddata:\n",
    "        if row1[5] == 'None':\n",
    "            both_match_with_none += 1\n",
    "        else:\n",
    "            marked_by_phrase2vec_only += 1\n",
    "\n",
    "for j, row2 in enumerate(goldarray):\n",
    "    in_phrase2vec = False\n",
    "    if j in candidate_lines:\n",
    "        continue\n",
    "    for i, row1 in enumerate(phrase2vecarray):       \n",
    "        if row1[1] == row2[1] and row1[2] == row2[2]:\n",
    "            in_phrase2vec = True if row1[5] != 'None' else False\n",
    "            break\n",
    "    if in_phrase2vec == False:\n",
    "        markedbycoach_only += 1\n",
    "        \n",
    "print('experment settings for phrase2vec:')\n",
    "\n",
    "print ('both hit:', both_match_with_theme)\n",
    "print('mismatch when marked by both phrase2vec and annotator', mismatch) \n",
    "print('both miss:',both_match_with_none)\n",
    "print('marked by only phrase2vec:', marked_by_phrase2vec_only)\n",
    "print('marked by coach only:', markedbycoach_only)      \n",
    "print ('candidate sentences: ', candidate_sentence_cnt)\n",
    "\n",
    "mat = confusion_matrix(annotator_themes, predicted_themes_by_phrase2vec,labels=themelabels)\n",
    "#print(themelabels)\n",
    "#print (mat)\n",
    "'''df_cm = pd.DataFrame(mat, index=themelabels, columns=themelabels)\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}) # font size\n",
    "plt.show()'''\n",
    "cohen_score = cohen_kappa_score(annotator_themes, predicted_themes_by_phrase2vec)\n",
    "print ('kappa score: ', cohen_score)\n",
    "\n",
    "#essaylevel theme existence experiment\n",
    "thememap = dict()\n",
    "for i,label in enumerate(themelabels):\n",
    "    thememap[label] = i\n",
    "\n",
    "phrasevalues = []\n",
    "annotatorvalues = []\n",
    "phrasevalues2 = [] #this array takes care of how many sentences contain a certain theme\n",
    "annotatorvalues2 = [] #this array takes care of how many sentences contain a certain theme\n",
    "for submission in essay_level_themes_phrase:\n",
    "    arr = [0]*len(themelabels)\n",
    "    arr2 = [0]*len(themelabels)\n",
    "    for theme in essay_level_themes_phrase[submission]:\n",
    "        arr[thememap[theme]] = 1\n",
    "        arr2[thememap[theme]] += 1\n",
    "    phrasevalues.extend(arr)\n",
    "    phrasevalues2.extend(arr2)\n",
    "\n",
    "for submission in essay_level_themes_annotation:\n",
    "    arr = [0]*len(themelabels)\n",
    "    arr2 = [0]*len(themelabels)\n",
    "    for theme in essay_level_themes_annotation[submission]:\n",
    "        arr[thememap[theme]] = 1\n",
    "        arr2[thememap[theme]] += 1\n",
    "    annotatorvalues.extend(arr)\n",
    "    annotatorvalues2.extend(arr2)\n",
    "print (len(phrasevalues), len(annotatorvalues))\n",
    "cohen_score = cohen_kappa_score(annotatorvalues, phrasevalues)\n",
    "print ('essay level kappa score: ', cohen_score)\n",
    "\n",
    "cohen_score = cohen_kappa_score(annotatorvalues2, phrasevalues2, weights='quadratic')\n",
    "print ('essay level kappa score (with weights): ', cohen_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
